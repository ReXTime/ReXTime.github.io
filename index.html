<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/t-rex-v4.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Jr-Jen Chen</a><sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Yu-Chien Liao</a><sup style="color:#6fbf73;">1</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Hsi-Che Lin</a><sup style="color:#6fbf73;">1</sup>,</span>
                  </span>
                  </div>
              
              <!-- New row of authors -->
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="FOURTH AUTHOR PERSONAL LINK" target="_blank">Yu-Chu Yu</a><sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">
                  <a href="FIFTH AUTHOR PERSONAL LINK" target="_blank">Yen-Chun Chen</a><sup style="color:#ffac33;">2</sup>,</span>
                <span class="author-block">
                  <a href="SIXTH AUTHOR PERSONAL LINK" target="_blank">Yu-Chiang Frank Wang</a><sup style="color:#6fbf73;">1</sup>,</span>
                </span>
              </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup style="color:#6fbf73;">1</sup>National Taiwan University,</span>
                    <span class="author-block"><sup style="color:#ffac33;">2</sup>Microsoft</span><br>
                    <span class="author-block">Submit to NeurIPS 2024 Datasets and Benchmarks Track</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- PDF Link. -->
                      <span class="link-block">
                        <!-- @PAN TODO: change links -->
                        <a href="https://arxiv.org/abs/2311.16502"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fas fa-file-pdf"></i>
                          </span>
                          <span>arXiv</span>
                        </a>
                      </span>
                      <span class="link-block">
                        <a href="https://huggingface.co/datasets/ReXTime/ReXTime"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <!-- <i class="far fa-images"></i> -->
                              <p style="font-size:18px">ü§ó</p>
                              <!-- üîó -->
                          </span>
                          <span>Dataset</span>
                        </a>
                      </span>
                      <span class="link-block">
                        <a href="https://github.com/ReXTime/ReXTime.git"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fab fa-github"></i>
                          </span>
                          <span>Code</span>
                          </a>
                      </span>
                      <!-- Dataset Link. -->                      
                      <!-- Visualization Link. -->
                      <span class="link-block">
                        <a href="https://eval.ai/web/challenges/challenge-page/2326/overview"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon has-text-white">
                              <!-- <p style="font-size:18px">üìñ</p> -->
                              <i class="fa-solid fa-medal"></i>
                          </span>
                          <span>EvalAI</span>
                        </a>
                      </span>
                    </div>
                  </div>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
        <div class="content has-text-centered">
          <img src="static/images/teaser_v9.jpg" alt="geometric reasoning" width="100%"/>
          <p>Our benchmark specializes in evaluating reasoning across time, i.e. video QA when question and answer each belongs to different time spans. ReXTime poses difficulties even for frontier MLLMs, as indicated by the large gap to human-level accuracy.</p>
        </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üîîNews</h2>
        <div class="content has-text-justified">
          <!-- <p>
            <b>üöÄ[2024-01-31]: We added Human Expert performance on the <a href="#leaderboard">Leaderboard</a>!üåü</b>
          </p>
          <p>
            <b>üî•[2023-12-04]: Our evaluation server for the test set is now available on <a href="https://eval.ai/web/challenges/challenge-page/2179/overview"><b>EvalAI</b></a>. We welcome all submissions and look forward to your participation! üòÜ</b>
          </p> -->
          <p>
            <b> [2024-06-06]: Submit to NeurIPS 2024 Datasets and Benchmarks Track</b>
          </p>
      </div>      
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce ReXTime, a benchmark designed to rigorously test AI models' ability to perform temporal reasoning within video events. Specifically, ReXTime focuses on reasoning across time, i.e. human-like understanding when the question and its corresponding answer occur in different video segments. This form of reasoning, requiring advanced understanding of cause-and-effect relationships across video segments, poses significant challenges to even the frontier multimodal large language models. To facilitate this evaluation, we develop an automated pipeline for generating temporal reasoning question-answer pairs, significantly reducing the need for labor-intensive manual annotations. Our benchmark includes 921 carefully vetted validation samples and 2,143 test samples, each manually curated for accuracy and relevance. Evaluation results show that while frontier large language models outperform academic models, they still lag behind human performance by a significant 14.3% accuracy gap. Additionally, our pipeline creates a training dataset of 9,695 machine generated samples without manual effort, which empirical studies suggest can enhance the across-time reasoning via fine-tuning. </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mmmu">
    <img src="static/images/t-rex-v4.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="mmmu" style="vertical-align: middle">ReXTime Benchmark</span>
  </h1>
  </div>
</section>
            
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have nearly matched human performance in various language and vision-language tasks. Notably, frontier MLLMs trained on web-scale proprietary datasets show impressive video understanding such as GPT-4o, Gemini-Pro-1.5 and Claude-Core. However, unlike LLMs which excel in text reasoning over long sequences, the cause-effect reasoning in MLLMs, especially in understanding long video events, remains under-explored. In an initial study, we identified a common shortcoming in the most advanced MLLMs -- they struggle with video question answering when the question and answer correspond to different time segments. As shown in Figure 1, the question‚ÄùHow can we cut up the tomato efficiently?'' and the answer ‚ÄúHold up a plate and sharpen the knife on the plate.'' each refers to separate segments. Surprisingly, a simple question like this can challenge leading MLLMs. Therefore, there is a pressing need for a benchmark to quantitatively assess video temporal reasoning. To address this, we introduce <b>ReXTime</b>, a benchmark to evaluate <b>Re</b>asoning-A<b>cross</b>-<b>Time</b> capabilities for video events. </p>
          <p>
            To develop ReXTime, we propose an LLM-assisted data generation pipeline that minimizes human effort and cuts costs from $300 to $135 per 1,000 QA pairs. The benchmark includes <b>921</b> validation and <b>2143</b> test samples, each rigorously curated by human annotators. Empirical evidence indicates that even proprietary frontier MLLMs are inadequate for temporal reasoning. For instance, humans can achieve 88.0% accuracy on VQA tasks, whereas the top-performing MLLM, OpenAI's GPT-4o, only reaches 73.7% as shown in Figure 1. A new benchmark such as ReXTime has the potential to significantly propel advancements in this field -- it effectively differentiates between model capabilities, and the state-of-the-art model has not yet saturated human-level accuracy. The additional 9695 unverified samples provide a training dataset that has significantly boosted an academic MLLM's temporal reasoning skills, lowering the entry bar for future research. Furthermore, we confirmed that \ours primarily contains <b>reasoning across time</b> questions, with the lowest question-answer overlap in time (QA-mIoU) compared to other video QA benchmarks. </p>
        </div>
        <!-- <div class="content has-text-centered">
          <img src="static/images/venn_v8.jpg" alt="algebraic reasoning" class="center">
        </div> -->
    </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Question Types in <b>ReXTime</b></h2>
        <div class="content has-text-justified">
          <p>
            We present the relationship and examples between the three categories of question we generated. ‚ÄúHaving dinner / Watching TV'' does not have strong causality and is classified in <i>sequential</i>, which often results in before / after questions. ‚ÄúGirls falls down'' shows strong causality with ‚ÄúThe girl is crying.'' but lacks human intention, is classified in <i>cause-effect</i>. ‚ÄúChopping tomato / Making a dish'' not only exists strong causal relations but also shows subjective deliberation, which is classified into <i>means-to-an-end</i>. </p>
        <div class="content has-text-centered">
          <img src="static/images/venn_v8.jpg" alt="algebraic reasoning" class="center">
          <p> Reasoning across time question-answer types and examples. The three categories are sequential, cause-effect, and means-to-an-end. </p>
        </div>
          
          
        </div>
    </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Data generation pipeline</h2>
        <div class="content has-text-justified">
          <p>
            To develop an efficient and effective pipeline, we have addressed two primary challenges: (1) the quality-diversity trade-off in LLM generation, and (2) the high cost of human labor for verification. Initially, prompting an (M)LLM to generate question-answer pairs often results in logically incorrect responses. While few-shot in-context learning enhances logical correctness, it reduces response diversity. We address this by moderating the MLLM with specific event attributes and temporal relations from a structured taxonomy. Additionally, although human verification is necessary to eliminate residual errors, we minimize costs by establishing criteria that allow the MLLM to self-assess the accuracy of its generated QAs. As a bonus feature, we evaluate video moment localization to assess whether an AI model accurately grounds its answers to the correct video segments. Please refer to our paper for more details. </p>
        <div class="content has-text-centered">
          <img src="static/images/main_v8.jpg" alt="algebraic reasoning" class="center">
          <p> In stage I, we collect event pairs from two video sources.
            In stage II, we score and categorize the event pairs into four relation types.
            In stage III, the (M)LLM generates question-answer pairs by our carefully written few-shot demonstrations.
            In stage IV, the LLM self-evaluate the generated samples to reduce the human verification cost. </p>
        </div>
          
          
        </div>
    </div>
    </div>
<!-- 
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparisons with Existing Benchmarks</h2>
        <div class="content has-text-justified">
          <p>
            To further distinguish the difference between <i>dataset</i> and other existing ones, we elaborate the benchmark details in Figure. 
            From the <i>breadth</i> perspective, the prior benchmarks are heavily focused on daily knowledge and common sense. 
            The covered image format is also limited. Our benchmark aims to cover college-level knowledge with 30 image formats including diagrams, 
            tables, charts, chemical structures, photos, paintings, geometric shapes, music sheets, medical images, etc. 
            In the <i>depth</i> aspect, the previous benchmarks normally require commonsense knowledge or simple physical or temporal reasoning. 
            In contrast, our benchmark requires deliberate reasoning with college-level subject knowledge.
        </p>
        <div class="content has-text-centered">
          <img src="static/images/main_v8.jpg" alt="algebraic reasoning" class="center">
          <p> Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason.</p>
        </div>
          
          
        </div>
    </div>
    </div> -->

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Comparisons with Existing Benchmarks</h2>
        <!-- <div class="content has-text-justified">
          <p>
            
        </p> -->
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/dataset_comp1.png" alt="algebraic reasoning" width="95%"/>
              <p> We compare ReXTime with related datasets on temporal reasoning or moment localization, highlighting our uniqueness. ReXTime covers features from all similar video QA tasks. Notably, ‚Äúreasoning-across-time‚Äù emphasizes the cause and effect understanding between visual events.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/dataset_comp2.png" alt="arithmetic reasoning" width="40%"/>
              <p> Our comparison focuses on datasets with both question queries and moment localization features. We present a comprehensive report detailing the number of temporal reasoning samples on each split, certificate length (C.L.) and Question-Answer mean Intersection over Union (QA-mIoU) respectively. A higher average certificate length indicates that a model needs to reason across a longer duration in a video. A lower QA-mIoU indicates smaller intersection of question span and answer span, requiring the model to reason across different time segments in a video. From the qualitative measures, ReXTime serves as a better benchmark to evaluate the reasoning across time capability. (<span>&#8224;</span>: Only counts temporal reasoning QA pairs. See supplementary in paper for details.)</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
